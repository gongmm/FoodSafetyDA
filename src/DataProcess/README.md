# 数据处理主题提取事件提取

* `file_process`：文件处理类
  * 文件编码格式转换：GBK -> utf-8
  * 读取csv保存为单独的txt文件(去掉数字、字母、制表符、停用词)
  * 读取csv保存为一个txt文件(去掉数字、字母、制表符、停用词)
  * 遍历指定目录，显示目录下的所有文件名
  * 读取文件内容并打印
  * 去掉文本中的html标签
  * 构建语料库
* `random_data.py`：从所有的新闻文本中选取choose_num条新闻，用于标记
* `lda_class.py`：LDA主题提取类，利用lda包下的LDA模块进行模型训练、主题提取
  * 读取txt文件内容建立语料库
  * LDA模型训练
  * 打印lda主题词
  * 存储每篇文档对应的topic_id
  * 在原文档后加一列表示每篇文档对应的主题编号
  * 绘制主题词分布图（不建议调用，由于词数量过多，绘制需要耗费大量时间）
  * 绘制文档——主题分布图
  * 将话题关键词存入单独的csv文件 ['topic_id', 'topic_word']
* `gensim_lda_topic_analysis.py`：利用gensim包下的LDA模块进行模型训练、主题提取
* `lda_test.py`：对各种不同的LDA模型进行尝试
* `skl_lda.py`：利用sklearn包下的LDA模块进行模型训练、主题提取
* `topic_analysis.py`：根据困惑度，确定主题数
* `word_2_vec.py`：word2vec模型训练，对词进行向量化
* `extract_cluster_result.py`：提取聚类结果
  * 获得层次聚类后的结果
  * 获得某个主题的聚类结果
  * 保存聚类结果，将主题下的文档按照事件分类存储 `topic_i/events/event_j/doc...`
* `extract_keyword.py`：提取事件关键词
  * 获得路径下的所有文件，包括子文件夹下的文件
  * 提取文本中的前十个关键词
  * 提取一个事件中的关键词，将描述该事件的文档拼接然后进行关键词提取
  * 提取话题中所有事件的关键词
  * 保存话题中各个事件的关键词

## 文件说明 ##

* `corpus`：存放语料库、停用词
* `model`：存放中间模型
* `result`：存放主题提取结果

## 运行

```shell
$ python ***.py
```

## 结果

- 数据处理：数据清洗、去停用词、构建语料库
- LDA主题提取
- 文本向量化
- 事件提取

