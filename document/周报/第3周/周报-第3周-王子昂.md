# 第3周周报——王子昂 #

## 初步确定趋势分析步骤 ##

与余连玮商量后，制定了趋势分析的具体计划：

1. 获取2018年一整年的食品安全相关的数据。
2. 进行数据处理后，送入LDA模型中训练，得到一年中所有的主题及所有文档对应的主题。
3. 提取出一年中Top-N的主题，统计一年中这些主题的热度趋势
4. 对这些主题做后续的热度预测。

## 爬取食品伙伴网 ##

> 爬取食品资讯板块[中国食品栏目](http://news.foodmate.net/guonei/)2018年的新闻，利用scrapy框架，共爬取到10304条数据

* 通过分析URL规律实现分页爬取

  http://news.foodmate.net/guonei/list_250.html

  * 250代表第250页的内容，通过该数字的增加实现各个页面的爬取

* 通过xpath获取列表页面的信息：标题、发布时间、新闻详情页的连接

  ![1564240378772](周报-第3周-王子昂.assets/1564240378772.png)

  * html界面如下，通过xpath解析信息

  ![1564240440285](周报-第3周-王子昂.assets/1564240440285.png)

  ```python
  li_list = sel.xpath("//div[@class='catlist']/ul/li[@class='catlist_li']")
      for content in li_list:
          link = content.xpath("./a/@href").extract()[0]
          title = content.xpath("./a/@title").extract()[0]
          time = content.xpath("./span/text()").extract()[0]
  ```

* 对于时间符合要求（2018年）的数据，利用获得的详情页URL再进一步爬取新闻内容、行业、标签、地点，利用xpath进行解析

  ![1564240766832](周报-第3周-王子昂.assets/1564240766832.png)

* 在pipeline中进行数据存储，能够保存为表格、json、存入Mongo数据库

  存入数据库中的数据如图中所示：

  ![1564241077194](周报-第3周-王子昂.assets/1564241077194.png)

  导出为表格如下：

  ![1564241186526](周报-第3周-王子昂.assets/1564241186526.png)

## 爬取国家食药监督总局 ##

> 爬取[食品、保健食品欺诈和虚假宣传整治板块](http://samr.cfda.gov.cn/WS01/CL1972/index.html)2018年的新闻，共爬取到6760条数据

* 使用Selenium模拟浏览器行为进行爬取

* 利用BeautifulSoup对网页进行解析，获得相应的内容

* 将内容存储到csv文件中

  ![1564241889726](周报-第3周-王子昂.assets/1564241889726.png)

## 爬取人民日报数据库 ##

> 爬取[人民日报数据库](http://data.people.com.cn/)关键词搜索结果，通过搜索“食品安全”关键词，得到840条数据

* 人民日报数据库需要付费登录才能访问，但通过武大图书馆可以直接进行访问，因此分析了武大图书馆提供的访问人民日报数据库的链接，通过Selenium模拟浏览器行为，打开浏览器进行访问，得到登陆状态，再进行进一步的爬取

* 人民日报搜索的URL较为特殊，采用附加Json的形式

  `http://data.people.com.cn/sc/ss?type=1&qs=%7B%22cds%22%3A%5B%7B%22cdr%22%3A%22AND%22%2C%22cds%22%3A%5B%7B%22fld%22%3A%22title%22%2C%22cdr%22%3A%22OR%22%2C%22hlt%22%3A%22true%22%2C%22vlr%22%3A%22OR%22%2C%22val%22%3A%22%E9%A3%9F%E5%93%81%E5%AE%89%E5%85%A8%22%7D%2C%7B%22fld%22%3A%22subTitle%22%2C%22cdr%22%3A%22OR%22%2C%22hlt%22%3A%22true%22%2C%22vlr%22%3A%22OR%22%2C%22val%22%3A%22%E9%A3%9F%E5%93%81%E5%AE%89%E5%85%A8%22%7D%2C%7B%22fld%22%3A%22introTitle%22%2C%22cdr%22%3A%22OR%22%2C%22hlt%22%3A%22true%22%2C%22vlr%22%3A%22OR%22%2C%22val%22%3A%22%E9%A3%9F%E5%93%81%E5%AE%89%E5%85%A8%22%7D%5D%7D%5D%2C%22obs%22%3A%5B%7B%22fld%22%3A%22dataTime%22%2C%22drt%22%3A%22DESC%22%7D%5D%7D`

  经过分析，URL可分解为下面的结构

  ```python
  SpecificBaseUrl = r"http://data.people.com.cn/sc/ss?qs="
  
  GlobalJsonPart = {"clds": "23",
                    "cds":
                        [{"cdr": "AND",
                          "cds":
                          [
                          {"fld": "title", "cdr": "OR", "hlt": "true", "vlr": "AND", "val": ""},
                          {"fld": "subTitle", "cdr": "OR", "hlt": "false", "vlr": "AND", "val": ""},
                          {"fld": "introTitle", "cdr": "OR", "hlt": "false", "vlr": "AND", "val": ""},
                          {"fld": "contentText", "cdr": "OR", "hlt": "true", "vlr": "AND",
                                "val": ""}]}],
                    "obs": [{"fld": "dataTime", "drt": "DESC"}]}
  ```

* 由于只需要2018年的数据，为减少爬取的数据量，通过人工审阅，2018年的数据分布在30页到60页，请求的页数可直接附加在请求的URL中

* 通过对搜索页面进行解析，提取各个结果详情页的URL存入链表

* 对每一个URL进行访问获取详情内容

* 由于该数据库存在反爬虫机制，同一个IP多次访问会导致服务器禁止IP访问，通过更换User Agent，建立IP池，来防止这种情况的发生，一旦出现请求错误，更换IP同时`sleep`一段时间再重新进行访问

![1564316945014](周报-第3周-王子昂.assets/1564316945014.png)

## 爬取微信公众号历史文章 ##

> 爬取“中国食品网”、“中国食品安全报”、“中国食品安全网”三个公众号2018年发布的文章，共得到5120条数据

### 常用爬取方式 ###

* 通过[微信公众号平台](https://mp.weixin.qq.com/)新建一篇文章，添加超链接，在编辑超链接中可以搜索微信公众号，通过chrome中的Network工具可以获得对应的网络请求，获得搜索接口和文章接口。使用Selenium模拟浏览器行为登录微信公众号，获取登录之后的cookies信息，并保存到本地文本中，通过获得的接口进行爬取，最后进行存储。由于其复杂的反爬虫机制，在尝试的过程中经常因为多次访问被禁，需要输入验证码由于对验证码的处理比较复杂，因此放弃了这种方法。
* 通过搜狗搜索微信的功能，搜狗搜索提供微信栏目可以在其中搜索微信公众号以及文章，在尝试的过程中发现，其搜索微信公众号的功能存在问题，任何时间进入都需要输入验证码，人工输入验证码也不能通过，因此放弃了这种做法。

### 采用的策略 ###

* 在GitHub中发现了一个微信爬虫工具[wechat-spider](https://github.com/striver-ing/wechat-spider)，通过[mitmproxy](https://www.cnblogs.com/yunlongaimeng/p/9617708.html)代理拦截客户端查询文章的请求解析其数据实现数据的获取，可以得到公众号数据，文章列表数据、文章数据、阅读点赞评论数据
* 首先需要安装redis和mysql
* 其次安装mitmproxy证书，下载mitmproxy然后运行`mitmdump`命令获得正数
* 之后配置代理，正确配置config.yaml
* 建立数据库，在wechat_account_task表中下发采集任务，添加公众号的`_biz`信息（可从公众号的URL中获得）
* 启动程序，打开微信客户端选择要爬取的公众号即可自动进行爬取

## 爬取澎湃新闻 ##

> 爬取[澎湃新闻](https://www.thepaper.cn/)关键词搜索“食品安全”的结果及其对应的内容，共得到1214条结果，主要基于原有代码进行修改

* 通过分析可得到澎湃新闻搜索的URL形式如下`'https://www.thepaper.cn/load_search.jsp?k=' + keyword + '&pagesize=10&searchPre=all_0:&orderType=1&pageidx=page`

  通过替换关键词和页数即可得到所有的搜索结果页

* 首先通过request获得网页信息，利用xpath提取搜索页的信息包括标题、发布时间，以及内容详情页的URL

* 对于每一个搜索结果，通过得到的URL进行访问，通过xpath解析得到文章详情

* 将内容存储到Mongo数据库中，在导出为CSV

![1564319922611](周报-第3周-王子昂.assets/1564319922611.png)

## 爬取中国食品安全网 ##

> 爬取[中国食品安全网](http://www.cfsn.cn/)首页各个类别下的所有新闻，基于原有代码进行修改

* 利用request获得URL对应的html页面
* 利用BeautifulSoup对网页进行解析，获得相应的内容
* 首先获取首页各个栏目的URL
* 其次判断每一类别下有几页，获取每一页的链接
* 之后获取每一页下符合时间条件的新闻链接
* 再爬取每一条新闻的具体内容
* 将结果存入csv

## 爬取食品论坛搜索结果 ##

> 爬取[食品论坛](http://bbs.foodmate.net)关键词搜索的结果，并爬取帖子中的所有内容

- 利用使用Selenium模拟浏览器行为进行爬取
- 利用BeautifulSoup对网页进行解析，获得相应的内容
- 首先获得搜索页的信息包括标题、发布时间，以及帖子详情页的URL，再通过页面分析得到下一个搜索页的URL
- 对于每一个搜索结果，对帖子的具体内容进行爬取，依然采用分页爬取的策略，获得帖子中的所有信息，以此来分析话题的热度
- 将内容存储到csv文件中

![1564308358980](周报-第3周-王子昂.assets/1564308358980.png)

## 爬取微博搜索结果 ##

> 爬取微博相关关键词的搜索结果，通过分析得到的数据，我们认为微博的数据更适合于进行对某一事件进行舆情分析，因此并没有把爬取到的数据用于主题和事件提取

* 利用微博API进行爬取，请求格式如下

  ```python
  start_url = 'https://m.weibo.cn/api/container/getIndex?'
  params = {
      'containerid': '100103type=1&q=' + keyword,
      'page_type': 'searchall',
      'page': self.page
  	}
  ```

  返回的JSON格式如下：

  ![1564320561124](周报-第3周-王子昂.assets/1564320561124.png)

  可见`mblog`中存放微博相关信息

* 通过发送请求获得返回的json文件，对json文件进行解析，得到微博信息，包括：内容、评论数、点赞数、转发数等

* 最后将内容保存到表格中

![1564320968217](周报-第3周-王子昂.assets/1564320968217.png)

## 爬取知乎话题 ##

> 爬取知乎食品安全相关的话题，通过分析得到的数据，我们认为知乎的数据更适合于进行对某一事件进行舆情分析，因此并没有把爬取到的数据用于主题和事件提取

* 利用知乎API进行爬取，请求格式如下

  ```python
  start_url = 'https://www.zhihu.com/api/v4/search_v3?'
  params = {
      't': 'general',
  	'q': keyword,
      'correction': 1,
      'offset': 20,
      'limit': 20,
      'lc_idx': 26
  }
  ```

  返回的Json格式如下：

  ![1564321411797](周报-第3周-王子昂.assets/1564321411797.png)

  * `content`中可以获得内容信息
  * `question`中可以看到不同的问题信息
  * 每一个`object`对应一个话题

* 通过发送请求获得返回的json文件，对json文件进行解析，得到微博信息，包括：内容、评论数、点赞数、转发数等

* 最后将内容保存到表格中

![1564321593867](周报-第3周-王子昂.assets/1564321593867.png)

## 最终结果 ##

* 最终选择食品伙伴网、国家食药监督局、人民日报数据库、微信公众号、澎湃新闻的爬取结果进行主题分析和事件提取
* 决定利用微博、知乎、食品论坛的爬取结果进行后续的舆情分析
* 在进行数据爬取的过程中数据源的选择很关键，有的数据源数据量大、格式规范容易爬取、信息相关性高，而有的数据源则恰恰相反，在开始阶段分析内容、确定合适的数据源可以为后续的爬取过程节约很多事件
* 大多数网站都会建立一定的反爬虫机制，尤其是当爬取大量数据时，一般可通过建立IP池，更换UserAgent信息，模拟浏览器行为，对于需要输入验证码的情况，一般可以采用利用图像识别得到二维码在进行输入，或是得到验证码图片反馈给用户让用户进行输入识别